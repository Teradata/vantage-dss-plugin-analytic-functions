{
  "json_schema_major_version": "1",
  "json_schema_minor_version": "2",
  "json_content_version": "1",
  "function_name": "TextTokenizer",
  "function_version": "3.7",
  "function_type": "non-driver",
  "function_alias_name": "TextTokenizer",
  "function_r_name": "aa.text.tokenizer",
  "short_description": "Text classification SQL/MR function",
  "long_description": "The TextTokenizer function extracts English, Chinese, or Japanese tokens from text. Examples of tokens are words, punctuation marks, and numbers. Tokenization is the first step of many types of text analysis. This function can be used with real-time applications. See AMLGenerator.",
  "input_tables": [
    {
      "requiredInputKind": [
        "PartitionByAny"
      ],
      "isOrdered": false,
      "partitionByOne": false,
      "name": "input",
      "alternateNames": [],
      "isRequired": true,
      "rDescription": "The relation contains the text to be scanned",
      "description": "The relation contains the text to be scanned",
      "datatype": "TABLE_ALIAS",
      "allowsLists": false,
      "rName": "data",
      "useInR": true,
      "rOrderNum": 1
    },
    {
      "requiredInputKind": [
        "Dimension"
      ],
      "isOrdered": false,
      "partitionByOne": false,
      "name": "dict",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "The relation that contains the dictionary for segementing words.",
      "description": "The relation that contains the dictionary for segementing words.",
      "datatype": "TABLE_ALIAS",
      "allowsLists": false,
      "rName": "dict.data",
      "useInR": true,
      "rOrderNum": 2
    }
  ],
  "argument_clauses": [
    {
      "targetTable": [
        "input"
      ],
      "checkDuplicate": true,
      "allowedTypes": [],
      "allowedTypeGroups": [
        "ALL"
      ],
      "requiredLength": 1,
      "matchLengthOfArgument": "",
      "allowPadding": true,
      "name": "TextColumn",
      "alternateNames": [],
      "isRequired": true,
      "rDescription": "Specifies the name of the input table column that contains the text to tokenize",
      "description": "Specify the name of the input table column that contains the text to tokenize.",
      "datatype": "COLUMNS",
      "allowsLists": false,
      "rName": "text.column",
      "useInR": true,
      "rOrderNum": 3
    },
    {
      "targetTable": [
        "input"
      ],
      "checkDuplicate": true,
      "allowedTypes": [],
      "allowedTypeGroups": [
        "ALL"
      ],
      "matchLengthOfArgument": "",
      "allowPadding": true,
      "name": "Accumulate",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": " Specifies the names of the input table columns to copy to the output table.",
      "description": " Specify the names of the input table columns to copy to the output table.",
      "datatype": "COLUMNS",
      "allowsLists": true,
      "rName": "accumulate",
      "useInR": true,
      "rOrderNum": 9
    },
    {
      "permittedValues": [
        "en",
        "zh_CN",
        "zh_TW",
        "jp"
      ],
      "defaultValue": "en",
      "isOutputColumn": false,
      "name": "InputLanguage",
      "alternateNames": [
        "Language"
      ],
      "isRequired": false,
      "rDescription": "Specifies the language of the text in text_column:  en (English, the default),  zh_CN (Simplified Chinese),  zh_TW (Traditional Chinese),  jp (Japanese)",
      "description": " Specify the language of the text in text_column : Option Description 'en' (Default) English 'zh_CN' Simplified Chinese 'zh_TW' Traditional Chinese 'jp' Japanese",
      "datatype": "STRING",
      "allowsLists": false,
      "rName": "language",
      "useInR": true,
      "rOrderNum": 4
    },
    {
      "permittedValues": [],
      "defaultValue": "/",
      "isOutputColumn": false,
      "name": "OutputDelimiter",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies the delimiter for separating tokens in the output. The default value is slash (/).",
      "description": " Specify the delimiter, a string, for separating tokens in the output. Default: '/' (slash)",
      "datatype": "STRING",
      "allowsLists": false,
      "rName": "output.delimiter",
      "useInR": true,
      "rOrderNum": 6
    },
    {
      "defaultValue": false,
      "name": "OutputByWord",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies whether to output one token in each row. The default value is 'false' (output one line of text in each row).",
      "description": "Specify whether to output one token in each row. Default: 'false' (Output one line of text in each row.)",
      "datatype": "BOOLEAN",
      "allowsLists": false,
      "rName": "output.byword ",
      "useInR": true,
      "rOrderNum": 7
    },
    {
      "permittedValues": [],
      "isOutputColumn": false,
      "name": "userDictionaryFile",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies the name of the user dictionary to use to correct results specified by the model. If you specify both this argument and a dictionary table (dict), then the function uses the union of user_dictionary_file and dict as its dictionary. That describes the format of user_dictionary_file and dict. Note: If the function finds more than one matched term, it selects the longest term for the first match.",
      "description": " Specify the name of the user dictionary to use to correct results specified by the model. If you specify both this argument and a dictionary table (dict), then the function uses the union of user_dictionary_file and dict as its dictionary. TextTokenizer Input describes the format of user_dictionary_file and dict. Note: If the function finds more than one matched term, it selects the longest term for the first match.",
      "datatype": "STRING",
      "allowsLists": false,
      "rName": "user.dictionary",
      "useInR": true,
      "rOrderNum": 8
    },
    {
      "permittedValues": [],
      "isOutputColumn": false,
      "name": "ModelFile",
      "alternateNames": [
        "Model"
      ],
      "isRequired": false,
      "rDescription": "Specifies the name of model file that the function uses for tokenizing. The model must be a conditional random-fields model and model_file must already be installed on the database. If you omit this argument, or if model_file is not installed on the database, then the function uses white spaces to separate English words and an embedded dictionary to tokenize Chinese text. Note: If you specify Language('jp'), the function ignores this argument.",
      "description": " Specify the name of model file that the function uses for tokenizing. The model must be a conditional random-fields model and model_file must already be installed on the ML Engine. If you omit this argument, or if model_file is not installed, then the function uses white spaces to separate English words and an embedded dictionary to tokenize Chinese text. Note: If you specify InputLanguage('jp') , the function ignores this argument.",
      "datatype": "STRING",
      "allowsLists": false,
      "rName": "model",
      "useInR": true,
      "rOrderNum": 5
    },
    {
      "permittedValues": [],
      "isOutputColumn": false,
      "matchLengthOfArgument": "",
      "allowPadding": true,
      "name": "SequenceInputBy",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies the LIST_VECTOR of column(s) that uniquely identifies each row of the input argument INPUT_ARG_NAME. The argument is used to ensure deterministic results for functions which produce results that vary from run to run.",
      "description": "Specifies the input column(s) that uniquely identify each row of input table. The format is 'input1:c1' where input1 refers to the alias used by the input table that contains such column named c1.",
      "datatype": "STRING",
      "allowsLists": true,
      "rName": "sequence.column",
      "useInR": true,
      "rOrderNum": 50
    }
  ]
}